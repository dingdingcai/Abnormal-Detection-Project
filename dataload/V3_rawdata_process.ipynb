{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Split multiple tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Files/DataSpace/single_abnormal/v3/task84_29\n",
      "/home/ubuntu/Dingding/Projects/abnormal-detection/Dataspace/v3/v3_splits_plus_task84_29/test/task84_29\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "\n",
    "####   Training set  ###########\n",
    "# task = 'task72'    ; sets = 'train'   # 同层同种\n",
    "# task = 'task74'    ; sets = 'test'     # 同层同种\n",
    "# task = 'task75_1'  ; sets = 'train'   # new sku\n",
    "# task = 'task75_2'  ; sets = 'train'   # new sku\n",
    "# task = 'task75_3'  ; sets = 'train'    # new sku\n",
    "# task = 'task75_4'  ; sets = 'train'   # new sku\n",
    "# task = 'task75_5'  ; sets = 'train'   # new sku\n",
    "# task = 'task75_6'  ; sets = 'train'   # new sku\n",
    "# task = 'task75_7'  ; sets = 'train'   # new sku\n",
    "# task = 'task75_8'  ; sets = 'train'   # new sku\n",
    "# task = 'task75_9'  ; sets = 'train'    # new sku\n",
    "# task = 'task75_10' ; sets = 'test'    # new sku\n",
    "# task = 'task75_11' ; sets = 'train'    # new sku\n",
    "# task = 'task75_12' ; sets = 'train'    # new sku\n",
    "# task = 'task75_13' ; sets = 'test'    # new sku\n",
    "# task = 'task75_14' ; sets = 'train'   # new sku\n",
    "# task = 'task75_15' ; sets = 'valid'  # new sku\n",
    "# task = 'task76'    ; sets = 'train'  \n",
    "# task = 'task80_1'  ; sets = 'train'  \n",
    "# task = 'task80_2'  ; sets = 'train'  \n",
    "# task = 'task80_3'  ; sets = 'train'  \n",
    "# task = 'task80_4'  ; sets = 'train'  \n",
    "# task = 'task80_5'  ; sets = 'train'  \n",
    "# task = 'task80_6'  ; sets = 'train'  \n",
    "# task = 'task80_7'  ; sets = 'train'  \n",
    "# task = 'task80_8'  ; sets = 'test'  \n",
    "# task = 'task84'\n",
    "task = 'task85'\n",
    "\n",
    "base_path = os.path.abspath('../')\n",
    "\n",
    "# old_task = 'task80_1'\n",
    "old_plan = 'v3_splits_plus_' + old_task\n",
    "new_plan = 'v3_splits_plus_' + task\n",
    "old_task = task\n",
    "\n",
    "\n",
    "source_basepath = '/home/ubuntu/Files/DataSpace/single_abnormal/v3'\n",
    "\n",
    "old_split_path = os.path.join(base_path, '/Dataspace', old_plan)\n",
    "new_split_path = os.path.join(base_path, '/Dataspace', new_plan)\n",
    "\n",
    "if os.path.exists(new_split_path):\n",
    "    shutil.rmtree(new_split_path)\n",
    "    \n",
    "shutil.copytree(old_split_path, new_split_path, symlinks=True)\n",
    "\n",
    "src_task_path = os.path.join(source_basepath, task)\n",
    "dst_task_path =  os.path.join(new_split_path, sets, task)\n",
    "\n",
    "os.symlink(src_task_path, dst_task_path)\n",
    "print(src_task_path)\n",
    "print(dst_task_path)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate the symlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, abnormal: 126879, normal: 215706\n",
      "test, abnormal: 27261, normal: 41257\n",
      "valid, abnormal: 9618, normal: 13610\n",
      "Total:427668\n"
     ]
    }
   ],
   "source": [
    "import shutil, os, glob, re, json\n",
    "import numpy as np\n",
    "\n",
    "dataspace_path = os.path.join(base_path, '/Dataspace')\n",
    "\n",
    "source_path = os.path.join(dataspace_path, 'v3_splits_plus_task84')\n",
    "target_path = os.path.join(dataspace_path, 'v3_OriginalSet_task84_with123')\n",
    "\n",
    "\n",
    "def _recursive_list(subpath):\n",
    "    return sorted(os.walk(subpath, followlinks=True), key=lambda tpl: tpl[0])\n",
    "\n",
    "def generate_symlinks(source_path, target_path):\n",
    "    counter = 0\n",
    "    datasets = ['train', 'test', 'valid']\n",
    "    \n",
    "    if os.path.exists(target_path):\n",
    "        shutil.rmtree(target_path)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        abnormal_nb = 0\n",
    "        normal_nb = 0\n",
    "        dataset_source = os.path.join(source_path, dataset)    # walk through dataset\n",
    "        for dirpath, _, file_list in _recursive_list(dataset_source):\n",
    "            if len(file_list) != 0:                            # find images and jsons\n",
    "                filename = []\n",
    "                for file in sorted(file_list):\n",
    "                    temp = dirpath\n",
    "                    occup = temp.replace(dataset, '***')\n",
    "                    subs = re.split('[***]', occup)[-1]\n",
    "                    elements = re.split('[/]', subs)\n",
    "                    prefix_str = ''\n",
    "\n",
    "                    for ele in elements:\n",
    "                        prefix_str += ele + '_'\n",
    "                    prefix_str = prefix_str[1:-1]  \n",
    "\n",
    "                    if file.endswith('.jpg'):                     # walk through images\n",
    "                        json_file = file.replace('.jpg', '.json')\n",
    "                        json_file = os.path.join(dirpath, json_file)\n",
    "                        json_str = ''\n",
    "                        if os.path.exists(json_file):             # if json file avaliable\n",
    "                            f = open(json_file)\n",
    "                            r = f.read()\n",
    "                            f.close()\n",
    "                            if not r:\n",
    "                                continue\n",
    "                            json_content = json.loads(r)[2]\n",
    "                            for i in json_content:\n",
    "                                json_str += str(i)\n",
    "\n",
    "                        if '1' in json_str:\n",
    "                            new_image_name = prefix_str + '_' + file.replace('.jpg', '_' + json_str + '.jpg') \n",
    "                            linked_dataset = os.path.join(target_path, dataset, '1_abnormal')\n",
    "                            abnormal_nb += 1\n",
    "                        else:\n",
    "                            new_image_name = prefix_str + '_' + file.replace('.jpg', '_0000000.jpg')\n",
    "                            linked_dataset = os.path.join(target_path, dataset, '0_normal')\n",
    "                            normal_nb += 1\n",
    "\n",
    "                        if not os.path.exists(linked_dataset):\n",
    "                            os.makedirs(linked_dataset)\n",
    "\n",
    "                        linked_img_file = os.path.join(linked_dataset, new_image_name)\n",
    "                        source_img_file = os.path.join(dirpath, file)\n",
    "\n",
    "                        if '同种不同面' not in linked_img_file:\n",
    "                            os.symlink(source_img_file, linked_img_file)\n",
    "                            counter += 1\n",
    "\n",
    "        if not os.path.exists(os.path.join(target_path, dataset, '0_normal')):\n",
    "            os.makedirs(os.path.join(target_path, dataset, '0_normal'))\n",
    "        if not os.path.exists(os.path.join(target_path, dataset, '1_abnormal')):\n",
    "            os.makedirs(os.path.join(target_path, dataset, '1_abnormal'))\n",
    "        print('{}, abnormal: {}, normal: {}'.format(dataset, abnormal_nb, normal_nb))\n",
    "    return counter\n",
    "\n",
    "counter = generate_symlinks(source_path=source_path, target_path=target_path)\n",
    "print('Total:{}'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand training abnormal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Expand_Abnormal(old_basedir, new_basedir, dataset):\n",
    "    \"\"\"\n",
    "    expanding the dataset for mitigating the imbalance problem \n",
    "    \"\"\"\n",
    "    old_set = os.path.join(old_basedir, dataset)\n",
    "\n",
    "    new_set = os.path.join(new_basedir, dataset)\n",
    "    if not os.path.exists(new_set):\n",
    "        raise(new_set + 'does not exists!')\n",
    "\n",
    "    abnormal = glob.glob(os.path.join(old_set, '1_abnormal/*.jpg'))\n",
    "    normal = glob.glob(os.path.join(old_set, '0_normal/*.jpg'))\n",
    "    \n",
    "    len_norm = len(normal)\n",
    "    len_abnorm = len(abnormal)\n",
    "\n",
    "    for i in range(len_norm - len_abnorm):\n",
    "        rand_num = np.random.choice(len_abnorm)\n",
    "        chosen_img = abnormal[rand_num]\n",
    "        img_name = re.split('[/.]', chosen_img)[-2]\n",
    "        linked_path = new_set + '/1_abnormal/' + str(i) + '_' + img_name  + '.jpg'\n",
    "        os.symlink(os.readlink(abnormal[rand_num]), linked_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import shutil, os, glob, re, json\n",
    "import numpy as np\n",
    "\n",
    "orig_path =  os.path.join(base_path, '/Dataspace/v3_OriginalSet_585') \n",
    "exp_path  =  os.path.join(base_path, '/Dataspace/v3_ExpandedSet_585')\n",
    "\n",
    "dataset='train'\n",
    "train_orig = os.path.join(orig_path, dataset)\n",
    "train_exp  = os.path.join(exp_path, dataset)\n",
    "\n",
    "if not os.path.exists(exp_path):\n",
    "    !mkdir '/home/ubuntu/Dingding/Projects/abnormal-detection/Dataspace/v3/v3_ExpandedSet_585'\n",
    "else:\n",
    "    !rm -r '/home/ubuntu/Dingding/Projects/abnormal-detection/Dataspace/v3/v3_ExpandedSet_585/train'\n",
    "    \n",
    "!cp -r '/home/ubuntu/Dingding/Projects/abnormal-detection/Dataspace/v3/v3_OriginalSet_585/train' '/home/ubuntu/Dingding/Projects/abnormal-detection/Dataspace/v3/v3_ExpandedSet_585/'\n",
    "\n",
    "Expand_Abnormal(old_basedir=orig_path, new_basedir=exp_path, dataset=dataset)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
